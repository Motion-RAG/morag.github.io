<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Action-GPT</title>
  <meta name="description" content="Action-GPT">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">



</head>


  <body>

    <!-- <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>

    <a class="navbar-brand" href="http://localhost:4000/">Action-GPT</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<!-- <li><a href="http://localhost:4000/publications">Publications</a></li>
		<li><a href="http://localhost:4000/projects">Projects</a></li>
		<li><a href="http://localhost:4000/experience">Experience</a></li>
		<li><a href="https://sid2697.github.io/Blog_Sid">Blog</a></li> -->
	  </ul>
	</div>
  </div>
</div>
 -->

    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <h2 align="center">Action-GPT:<br />Leveraging Large-scale Language Models for Improved and
Generalized Zero Shot Action Generation</h2>

<p style="text-align: center;">
<a href="https://www.linkedin.com/in/sai-shashank-54288219b" style="color: #CC0000"> Sai Shashank Kalakonda</a>
      
<a href="https://shubhmaheshwari.github.io/website" style="color: #CC0000"> Shubh Maheshwari</a>
      
<a href="https://ravika.github.io" style="color: #CC0000"> Ravi Kiran Sarvadevabhatla</a>
      
</p>
<!-- <p style="text-align: center;"><a href="https://2022.acmmm.org/" style="color:#CC0000">ACMMM 2022</a></p> -->

<p style="text-align: center;">

<a href="https://arxiv.org/abs/2211.15603" class="btn">Paper</a>
      

<a href="" class="btn">Code</a>
      

</p>

<center id="overview">
<figure>
        <div id="teaser">
    <img src="http://localhost:4000/images/actiongpt/actiongpt_teaser2.gif" width="75%" />
        </div>
    <p>&nbsp;</p>
    <figcaption>
        Sample text conditioned action generations from a state of the art model TEACH (top row) and from our large language model based approach (Action-GPT-TEACH - bottom row). By incorporating large language models, our approach results in noticeably improved generation quality for seen and useen categories. The conditioning action phrases for seen categories are taken from BABEL whereas unseen action phrases were provided by a user.
    </figcaption>
</figure>
</center>

<hr style="border: 1px solid #555555;" />

<!-- [comment]: Atypical Sketch Content -->
<center>

<h3 align="center"> Abstract </h3>
</center>
<p>We introduce Action-GPT,</p>
<ul>
  <li>A plug and play framework for incorporating Large Language Models (LLMs) into text-based action generation models</li>
  <li>By carefully crafting prompts for LLMs, we generate richer and fine-grained descriptions of the action.</li>
  <li>We show that utilizing these detailed descriptions instead of the original action phrases leads to better alignment of text and motion spaces.</li>
  <li>Our experiments show qualitative and quantitative improvement in the quality of synthesized motions produced by recent text-to-motion models.</li>
  <li>Code, pretrained models and sample videos will be made available.</li>
</ul>

<hr style="border: 1px solid #555555;" />

<center id="overview">
<figure>
        <div>
    <img src="http://localhost:4000/images/actiongpt/ActGPT_Updated_Arch_v1.png" width="100%" style="box-shadow: 0px 0px 0px;" />
        </div>
    <p>&nbsp;</p>
    <figcaption>
        Action-GPT Overview: Given an action phrase, we first create a suitable prompt using an engineered prompt function. The result is passed to a large-scale language model (GPT-3) to obtain multiple action descriptions containing fine-grained body movement details. 
        The corresponding deep text representations are obtained using Description Embedder. The aggregated version of these embeddings is processed by the Text Encoder. During training, the action pose sequence is processed by a Motion Encoder. 
         The encoders are associated with a deterministic sampler (autoencoder) or a VAE style generative model.
        During training(shown with black), the latent text embedding and the latent motion embedding are aligned. During inference(shown in green), the sampled text embedding is provided to the Motion Decoder which outputs the generated action sequence.
    </figcaption>
</figure>
</center>

<hr style="border: 1px solid #555555;" />

<h3 align="center"> Comparisions </h3>

<center>
<figure>
        <div id="comparisions">
    <img src="http://localhost:4000/images/actiongpt/ActionGPT-Main-diagram.png" width="100%" />
        </div>
    <p>&nbsp;</p>
    <figcaption>
        Visual comparison of generated motion sequences across models trained on Action-GPT framework on BABEL dataset. Note that the generations using Action-GPT are well-aligned with the semantic information of action phrases. The example in right bottom-row shows latent space editing similar to  MotionCLIP. Action-GPT is better able to transfer the drink from mug style from standing to sitting pose.
    </figcaption>
</figure>
</center>

<!-- <hr style="border: 1px solid #555555;"> -->

<!-- 
[comment]: Code
<h3> Code </h3>
The code for this work is available on GitHub!<br>Link: <a href="https://github.com/pictionary-cvit/drawmon">pictionary-cvit/drawmon</a> -->

<!-- <h3> Acknowledgements </h3>
<p style="text-align: justify">
We wish to acknowledge grant from KCIS - TCS foundation.
</p> -->

<p> </p>

<h3 align="center"> Citation </h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{Action-GPT,
author="Kalakonda, Sai Shashank 
and Maheshwari, Shubh
and Sarvadevabhatla, Ravi Kiran",
title="Action-GPT: Leveraging Large-scale Language Models for Improved and
Generalized Zero Shot Action Generation",
booktitle = "arXiv",
year="2022"
}
</code></pre></div></div>

<p> </p>
<p> </p>

</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-5">

		  <p>&copy 2022 Nikhil Bansal. Site made with <a href="https://jekyllrb.com">Jekyll</a>.</p>
		   <p>  </p><p>


		</div>
		<div class="col-sm-5">
		</div>
    <div class="col-sm-5">
		</div>
		<div class="col-sm-5">
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
